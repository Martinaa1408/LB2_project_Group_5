# LAB2_project – SVM Method

This directory contains the **Support Vector Machine (SVM)** module of the project.  
It represents the **classification and optimization phase**, where the model is trained on the numerical features produced during the [`Features_extraction`](../Features_extraction/) stage.

The goal is to train a supervised classifier able to distinguish **signal peptide (SP)** from **non-SP** sequences based on physicochemical and compositional features extracted from the N-terminal region.

→ Notebook: `SVM_Method.ipynb`

---

## Objective
To build, tune, and evaluate an **SVM classifier** capable of predicting the presence of signal peptides in yeast proteins, achieving a robust trade-off between precision and recall.  
This module integrates:
- feature import from the previous notebook,  
- feature selection via Random Forest,  
- hyperparameter tuning through Grid Search,  
- full performance evaluation with confusion matrix and classification metrics.

---

## Conceptual Background
Support Vector Machines (SVMs) are **margin-based classifiers** that find an optimal hyperplane separating two classes.  
For non-linear data, the **RBF (Radial Basis Function) kernel** projects samples into a higher-dimensional space where separation becomes possible.

The optimization problem:
`math
\[
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_i \xi_i
\quad \text{s.t.} \quad y_i (w \cdot \phi(x_i) + b) \ge 1 - \xi_i
\]
`

where  
- \(C\) controls the regularization strength,  
- \(\phi(x)\) is the feature mapping function,  
- and \(\xi_i\) are slack variables allowing soft margins.
  
---

## Workflow Summary

| Step | Description | Output |
|------|--------------|---------|
| **1. Import features** | Load `ML_features.tsv` (8021 samples × 32 features) generated by the Feature Extraction module. | DataFrame with features + labels |
| **2. Preprocessing** | Apply `MinMaxScaler` to normalize variables between 0 and 1. | Scaled feature matrix |
| **3. Feature Selection (Random Forest)** | Train a Random Forest classifier (400 trees) and retain features above the median Gini importance. | 48 → 24 selected features |
| **4. Model definition** | Initialize `sklearn.svm.SVC()` classifier. | SVC object |
| **5. Grid Search CV** | Stratified 5-fold cross-validation on C ∈ [0.1, 1, 10], kernels ∈ [linear, poly, rbf, sigmoid], γ ∈ ['scale', 'auto']. | Best SVM parameters |
| **6. Evaluation** | Compute metrics (Accuracy, Precision, Recall, F1, MCC) and visualize confusion matrix. | Performance summary |

---

## Feature Selection (Random Forest)

Feature ranking was computed via **Gini importance** on biophysical features.  
The top-ranked variables contributing most to class separation were:

| Rank | Feature | Importance |
|------|----------|-------------|
| 1 | α-helix mean | 0.060 |
| 2 | Charge mean | 0.051 |
| 3 | Transmembrane max | 0.039 |
| 4 | Hydrophobicity max | 0.034 |
| 5 | α-helix max | 0.034 |
| 6 | Hydrophobicity mean | 0.033 |
| 7 | Transmembrane mean | 0.032 |
| 8 | Size mean | 0.030 |
| 9 | Size max | 0.024 |
| 10 | Charge max | 0.023 |

After selection, **24 out of 48 total features** were retained for model training.

A complementary plot (`RF_Gini_importance.png`) visualizes these rankings.

---

## Hyperparameter Optimization

### Parameter grid
| Parameter | Values tested | Meaning |
|------------|----------------|----------|
| **C** | [0.1, 1, 10] | Regularization strength (higher = stricter fit) |
| **Kernel** | ['linear', 'poly', 'rbf', 'sigmoid'] | Defines data mapping in feature space |
| **Gamma** | ['scale', 'auto'] | Kernel coefficient for RBF and polynomial kernels |

The **RBF kernel** with **C = 1** and **γ = 'scale'** achieved the best performance (val accuracy = 0.913).  
Validation accuracy vs. number of selected features is reported in `Accuracy_vs_Features.png` — showing that performance stabilizes after **k = 10** features.

---

## Evaluation Metrics

### Final Evaluation on Full Dataset
| Metric | Value |
|--------|-------:|
| **Accuracy** | 0.974 |
| **Precision** | 0.897 |
| **Recall** | 0.856 |
| **F1-score** | 0.876 |
| **MCC** | 0.861 |
| **Samples** | 8021 (SP+: 874 / SP–: 7147) |

**Confusion Matrix**

|                | Predicted Negative | Predicted Positive |
|----------------|-------------------:|-------------------:|
| **True Negative** | 7061 | 86 |
| **True Positive** | 126 | 748 |

---

## Classification Report

| Class | Precision | Recall | F1-score | Support |
|-------|------------|--------|-----------|----------|
| **Negative** | 0.98 | 0.99 | 0.99 | 7147 |
| **Positive** | 0.90 | 0.86 | 0.88 | 874 |
| **Macro avg** | 0.94 | 0.92 | 0.93 | — |
| **Weighted avg** | 0.97 | 0.97 | 0.97 | — |

The model demonstrates **high precision and recall**, especially for the majority class, while maintaining good sensitivity for SP detection.

---

## Visualization Outputs: 

| Plot | Description | File |
|------|--------------|------|
| **Confusion Matrix** | Visualization of classification outcomes. | `Plots/confusion_matrix.png` |
| **Validation Accuracy vs Top k Features** | SVM accuracy trend across selected feature subsets. | `Plots/Accuracy_vs_Features.png` |
| **Random Forest Gini Importances** | Barplot of top biophysical feature contributions. | `Plots/RF_Gini_importance.png` |
| **Precision–Recall Curve** | Trade-off between model precision and recall across thresholds. | `Plots/Precision_Recall_curve.png` |

---

## Output Files

| File | Description |
|------|--------------|
| **`final_svm_metrics.tsv`** | Summary of final performance metrics |
| **`confusion_matrix.png`** | Visualization of classification results |
| **`RF_Gini_importance.png`** | Feature importance plot |
| **`Accuracy_vs_Features.png`** | Feature selection performance trend |
| **`Precision_Recall_curve.png`** | PR diagnostic curve |

---

## Summary and Interpretation
- The **SVM (RBF kernel)** achieved a strong overall accuracy (**97.4%**) and MCC = 0.86, confirming its robustness.  
- Feature selection improved interpretability, showing that **α-helix propensity** and **charge distribution** are among the most discriminative features for SP recognition.  
- The **Random Forest + SVM pipeline** balances interpretability (feature relevance) and high predictive power.  
- The workflow is fully compatible with scikit-learn’s modular structure and can be extended to deep-learning benchmarks in later stages.

---

**Next step →** Integrate SVM results with the rule-based *von Heijne* method and perform final comparative evaluation in the [`Evaluation_and_Comparison`](../Evaluation_and_Comparison)  module.
